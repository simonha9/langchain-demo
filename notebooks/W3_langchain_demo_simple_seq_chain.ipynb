{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "45cf8c54-98e8-4df6-add0-681fc7c39b22",
   "metadata": {},
   "source": [
    "## Langchain demo of simple sequential chains\n",
    "\n",
    "In class, Alex showed y'all basic simple sequential chains, we will buid on top of this today.\n",
    "First, let's get all the config and imports stuff in order.\n",
    "\n",
    "1. Notice the the below code block has `import config`, find the `config.py` file (in the same directory) and put your openai API key in here.  If you do not have one yet, create a new account and refer to [here](https://help.openai.com/en/articles/4936850-where-do-i-find-my-secret-api-key) to find your openai API key. \n",
    "\n",
    "2. Once you have your key, update the key in `config.py`, it should have the format of `sk-XXXXXXXXXXXXXXXXXX` (number of X's is not exact so don't take that literally please).\n",
    "\n",
    "3. Run the block below once the above 2 are done so setup the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cfa9eccd-7af7-4a2c-90b5-a7540a601d21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " ········\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate, ChatPromptTemplate, HumanMessagePromptTemplate\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain import HuggingFaceHub, LLMChain\n",
    "from langchain.chains import SequentialChain, SimpleSequentialChain\n",
    "\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "from pydantic import BaseModel, Field, validator\n",
    "from typing import List\n",
    "import config\n",
    "import os\n",
    "\n",
    "os.environ['OPENAI_API_KEY'] = config.OPENAI_API_KEY\n",
    "\n",
    "# uncomment below if you want to use huggingface instead, when you run this, you will see an input box below, paste in your huggingface token in there\n",
    "# and wait to see \"........\"\n",
    "from getpass import getpass\n",
    "HUGGINGFACEHUB_API_TOKEN = getpass()\n",
    "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = HUGGINGFACEHUB_API_TOKEN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebb71929-3551-4e08-b992-2e1ef0baed5f",
   "metadata": {},
   "source": [
    "### What do we know?\n",
    "\n",
    "We can build a chain by piping the output from 1 prompt into the input of another.  We saw this in class with poems and analysis.\n",
    "\n",
    "The poem template was as such:\n",
    "```\n",
    "poem_template = \"\"\"\r\n",
    "You are a Poet. Given the title of a poem, write a haiku for that title.\r\n",
    "Haiku for: \r\n",
    "{title}\r\n",
    "\"\"\"\n",
    "\n",
    "and analysis template as such:\n",
    "```\n",
    "analysis_template = \"\"\"You are a poet critic. Given a haiku, it is your job to write an analysis.\r\n",
    "Poem analysis for:\r\n",
    "{haiku}\r\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "And we chained them together like this: `overall_chain = SimpleSequentialChain(chains=[poem_chain, analysis_chain], verbose=True)`\n",
    "\n",
    "Now this is pretty cool, but there is something subtle, see if you can spot it.\n",
    "Hint: it has something to do with the initial input variable: title, where did it go?``\n",
    "```\n",
    "o do so."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "880fa7a2-2454-4e07-9728-9eb7920b622d",
   "metadata": {},
   "source": [
    "#### The problem\n",
    "\n",
    "If you have not figured it out already, our initial `title` variable is missing from the output!\n",
    "In fact, imagine a chain as such\n",
    "\n",
    "foo -> bar -> baz\n",
    "\n",
    "where foo takes input a\n",
    "bar takes input b\n",
    "baz takes input c\n",
    "\n",
    "so it becomes foo(a) -> bar(b = foo(a)) -> baz(c = bar(foo(a)))\n",
    "\n",
    "Since every chain effectively ***consumes*** the input, that input is lost forever (or is it??) from the final output.  If you run the chain discussed in class, all we get is the analysis of the poem, we miss the poem title and the poem itself.\n",
    "\n",
    "We will recreate the situation below with food and restaurants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7b185a4a-9080-434c-96a8-7b7d6681269c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the model with temperature\n",
    "# temperature is a measure of how creative we want the model to be\n",
    "temperature = 0.5\n",
    "model = OpenAI(temperature=temperature)\n",
    "\n",
    "# uncomment below to test you have credits, if not go back up and use huggingface\n",
    "# name = model(\"say hi\")\n",
    "\n",
    "# uncomment below to use huggingface\n",
    "repo_id = \"google/flan-t5-xxl\"  # See https://huggingface.co/models?pipeline_tag=text-generation&sort=downloads for some other options\n",
    "hmodel = HuggingFaceHub(\n",
    "    repo_id=repo_id, model_kwargs={\"temperature\": temperature, \"max_length\": 64}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8d8a0749-ebe1-4ec6-bff9-866660b1aed6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new SimpleSequentialChain chain...\u001b[0m\n",
      "\u001b[36;1m\u001b[1;3mmexican fusion\u001b[0m\n",
      "\u001b[33;1m\u001b[1;3menchiladas, chile con queso, flautas,\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "enchiladas, chile con queso, flautas,\n"
     ]
    }
   ],
   "source": [
    "prompt_name = PromptTemplate(\n",
    "    input_variables = ['cuisine'],\n",
    "    template = \"Suggest a fancy name for a restaurant that serves {cuisine} food.\"\n",
    ")\n",
    "\n",
    "# change to hmodel if you want to use huggingface\n",
    "name_chain = LLMChain(llm=hmodel, prompt=prompt_name, output_key=\"restaurant_name\")\n",
    "\n",
    "prompt_items = PromptTemplate(\n",
    "    input_variables = ['restaurant_name'],\n",
    "    template = \"Suggest some menu items for {restaurant_name}\"\n",
    ")\n",
    "\n",
    "menu_items_chain = LLMChain(llm=hmodel, prompt=prompt_items, output_key=\"menu_items\")\n",
    "\n",
    "overall_chain = SimpleSequentialChain(chains=[name_chain, menu_items_chain], verbose=True)\n",
    "print(overall_chain.run('Mexican'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8a867d0-152a-484d-8f61-d388985fadea",
   "metadata": {},
   "source": [
    "If you run the above, notice the last line is just the menu items without the restaurant name!\n",
    "\n",
    "#### Solution\n",
    "\n",
    "In comes SequentialChains, these are more verbose than SimpleSequentialChains and include more information that we can use in other parts of our code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f13d1bb9-0ad7-4153-a38f-472a09ad100d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cuisine': 'Chinese',\n",
       " 'restaurant_name': 'chinese restaurant',\n",
       " 'menu_items': 'steamed fish in black bean sauce'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_chain = SequentialChain(\n",
    "    chains = [name_chain, menu_items_chain], \n",
    "    input_variables = ['cuisine'],\n",
    "    output_variables = ['restaurant_name', 'menu_items']\n",
    ")\n",
    "seq_chain({'cuisine': 'Chinese'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f5a066d-d759-4180-b8a8-787e7f86d303",
   "metadata": {},
   "source": [
    "And you can see, running the above gives us the cuisine, restaurant_name and menu_items as intended!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35642cd8-f121-45c0-89fb-861ff0773783",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
